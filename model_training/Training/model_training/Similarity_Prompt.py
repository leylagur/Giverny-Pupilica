import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HybridRecommendationEngine:
    """
    Hybrid model using semantic similarity + rule-based filtering
    """
    
    def __init__(self, dataset_path: str):
        """Initialize the recommendation engine"""
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # Multilingual model
        self.departments_df = None
        self.department_embeddings = None

        self.load_dataset(dataset_path)
        self.prepare_embeddings()
        
    def load_dataset(self, dataset_path: str):
        """Load department dataset - simple approach with error catching"""
        logger.info(f"Loading dataset from {dataset_path}")

        # Load CSV with ALL columns as string
        df = pd.read_csv(dataset_path, dtype=str)
        
        # Clean and prepare data
        df = df.dropna(subset=['Aciklama', 'bolum_adi'])
        
        # Manual ranking conversion with comprehensive error handling
        rankings = []
        valid_rows = []
        
        for idx, row in df.iterrows():
            ranking_value = row['2025_Taban_SÄ±ralama']
            
            try:
                # Skip if NaN or empty
                if pd.isna(ranking_value) or ranking_value == '':
                    continue
                    
                ranking_str = str(ranking_value).strip()
                
                # Skip any value containing comma
                if ',' in ranking_str:
                    continue
                    
                # Convert clean values
                if '.' in ranking_str:
                    clean_value = ranking_str.replace('.', '')
                else:
                    clean_value = ranking_str
                    
                ranking_int = int(clean_value)
                
                # Add to valid data
                rankings.append(ranking_int)
                valid_rows.append(idx)
                
            except:
                # Skip any problematic row silently
                continue
        
        # Create clean dataframe
        df_clean = df.iloc[valid_rows].copy()
        df_clean['ranking_2025'] = rankings
        df_clean = df_clean.reset_index(drop=True)
        
        self.departments_df = df_clean
        logger.info(f"Loaded {len(df_clean)} clean departments")
        
        if len(rankings) > 0:
            logger.info(f"Ranking range: {min(rankings)} - {max(rankings)}")
        print("CSV'den okunan ilk 5 ranking deÄŸeri:")
        print(df['2025_Taban_SÄ±ralama'].head().tolist())
        
        # Manual ranking conversion kÄ±smÄ±nda da debug ekle:
        print(f"Ä°lk 5 converted ranking: {rankings[:5]}")
        print(f"41 deÄŸeri nasÄ±l convert ediliyor: {df[df['2025_Taban_SÄ±ralama']=='41']['2025_Taban_SÄ±ralama'].iloc[0] if len(df[df['2025_Taban_SÄ±ralama']=='41']) > 0 else 'Yok'}")
        
        return df_clean
        
    def prepare_embeddings(self):
        """Create embeddings for all department descriptions"""
        logger.info("Creating embeddings for department descriptions...")
    
        self.departments_df = self.departments_df.reset_index(drop=True)
    
        descriptions = self.departments_df['Aciklama'].tolist()
        self.department_embeddings = self.model.encode(descriptions, show_progress_bar=True)
    
        logger.info("Embeddings created successfully")
    
    def extract_interests_and_ranking(self, user_input: str):
        """Parse user input to extract interests and ranking with NLP"""
        import re
        
        # Enhanced ranking patterns - sadece geÃ§erli formatlarÄ± kabul et
        ranking_patterns = [
            r'(?:YKS sÄ±ralamasi|sÄ±ralama|sÄ±ralamam):?\s*(\d+(?:\.\d{3})*k?)',
            r'sÄ±ralamam\s+(\d+(?:\.\d{3})*k?)',
            r'(\d+)\s*bin',  # BU SATIRI EKLE - "19 bin" yakalamak iÃ§in
            r'(\d+k?)\s*sÄ±ralama',
            r'(\d{1,3}(?:\.\d{3})+)',
            r'(\d{4,7})',
            r'sÄ±ralama.*?(\d+(?:\.\d{3})*k?)'
        ]
        
        ranking = None
        for pattern in ranking_patterns:
            ranking_match = re.search(pattern, user_input, re.IGNORECASE)
            if ranking_match:
                rank_str = ranking_match.group(1)
                
                try:
                    # SADECE GEÃ‡ERLÄ° FORMATLARI Ä°ÅLE
                    # VirgÃ¼l iÃ§eren formatlarÄ± IGNORE ET
                    if ',' in rank_str:
                        continue  # Bu formatÄ± atla, sonraki pattern'i dene
                    
                    # Handle 'k' notation (32k = 32000)
                    if 'k' in rank_str.lower():
                        ranking = int(float(rank_str.lower().replace('k', '')) * 1000)
                    elif '.' in rank_str and len(rank_str) > 4:
                        ranking = int(rank_str.replace('.', ''))  # 32.000 -> 32000
                    else:
                        ranking = int(rank_str)
                    
                    # GeÃ§erli ranking bulundu, dÃ¶ngÃ¼den Ã§Ä±k
                    if ranking:
                        break
                        
                except (ValueError, TypeError):
                    # Bu pattern Ã§alÄ±ÅŸmazsa sonrakini dene
                    continue
        
        # Extract career intentions and interests using NLP patterns
        interests_keywords = self.extract_career_interests(user_input)
        
        return ', '.join(interests_keywords), ranking
    
    def extract_career_interests(self, text: str):
        """Extract career interests and keywords from natural language with negative filtering"""
        import re
        
        text_lower = text.lower()
        extracted_interests = set()
        excluded_interests = set()  
        
        # Negative patterns - Ã¶nce bunlarÄ± kontrol et
        negative_patterns = {
            'teknoloji': [
                r'teknoloji.*?(?:sevmiyorum|istemiyorum|ilgilenmiyorum|olmasÄ±n|sevmem|alakalÄ±.*?olsun.*?istemiyorum)',
                r'(?:sevmiyorum|istemiyorum|ilgilenmiyorum|sevmem).*?teknoloji',
                r'teknoloji.*?ile.*?alakalÄ±.*?(?:olsun.*?istemiyorum|istemem)'
            ],
            'saÄŸlÄ±k': [
                r'saÄŸlÄ±k.*?(?:sevmiyorum|istemiyorum|ilgilenmiyorum|sevmem)',
                r'(?:sevmiyorum|istemiyorum|sevmem).*?saÄŸlÄ±k'
            ],
            'matematik': [
                r'matematik.*?(?:sevmiyorum|kÃ¶tÃ¼yÃ¼m|zor|sevmem)',
                r'(?:sevmiyorum|kÃ¶tÃ¼yÃ¼m|sevmem).*?matematik'
            ]
        }
        
        # Check negative patterns first
        for category, patterns in negative_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    excluded_interests.add(category)
                    break
        
        # Career intention patterns (pozitif)
        career_patterns = {
            'saÄŸlÄ±k': [
                r'saÄŸlÄ±k.*?(?:sektÃ¶r|alan|Ã§alÄ±ÅŸ)',
                r'hasta.*?bakÄ±m',
                r'tÄ±p.*?alan'
            ],
            'sanat': [
                r'(?:yaratÄ±cÄ±|sanat).*?(?:iÅŸ|alan|Ã§alÄ±ÅŸ)',
                r'tasarÄ±m.*?(?:yapma|alan)',
                r'gÃ¶rsel.*?(?:sanat|tasarÄ±m)'
            ],
            'teknoloji': [
                r'teknoloji.*?(?:sektÃ¶r|alan|Ã§alÄ±ÅŸ)',
                r'yazÄ±lÄ±m.*?(?:geliÅŸtir|alan)',
                r'bilgisayar.*?(?:program|alan)'
            ],
            'spor': [
                r'spor.*?(?:alan|aktivite|sektÃ¶r)',
                r'antrenÃ¶r.*?(?:olma|Ã§alÄ±ÅŸ)'
            ]
        }
        
        # Check positive career patterns
        for category, patterns in career_patterns.items():
            if category not in excluded_interests:  # Sadece exclude edilmemiÅŸleri ekle
                for pattern in patterns:
                    if re.search(pattern, text_lower):
                        extracted_interests.add(category)
                        break
        
        # Direct keyword matching
        context_keywords = {
            'saÄŸlÄ±k': ['saÄŸlÄ±k', 'hasta', 'tedavi', 'tÄ±p', 'hemÅŸire'],
            'teknoloji': ['teknoloji', 'bilgisayar', 'yazÄ±lÄ±m', 'program'],
            'sanat': ['sanat', 'tasarÄ±m', 'yaratÄ±cÄ±', 'gÃ¶rsel', 'grafik'],
            'spor': ['spor', 'fitness', 'antrenÃ¶r', 'egzersiz']
        }
        
        for category, keywords in context_keywords.items():
            if category not in excluded_interests:  # Exclude edilmemiÅŸleri kontrol et
                for keyword in keywords:
                    if keyword in text_lower:
                        extracted_interests.add(category)
                        break
        
        # Return final interests excluding negatives
        final_interests = extracted_interests - excluded_interests
        return list(final_interests) if final_interests else ['genel']
    
    def filter_negative_interests(self, results: list, user_input: str):
        """Remove departments that match negative interests"""
        import re
        
        user_lower = user_input.lower()
        filtered_results = []
        
        # negatif ve pozitif keywordler gemini modeline sorgu atarak oluÅŸturulmuÅŸtur Keyword_ask.py klasÃ¶rÃ¼nde yazmaktadÄ±r
        negative_filters = {
            'teknoloji': {
                'patterns': [
                    r'teknoloji.*?(?:istemiyorum|sevmiyorum|olmasÄ±n|sevmem)',
                    r'teknoloji.*?ile.*?alakalÄ±.*?(?:olsun.*?istemiyorum|istemem)',
                    r'bilgisayar.*?(?:istemiyorum|sevmiyorum|sevmem)',
                    r'matematik.*?(?:sevmiyorum|kÃ¶tÃ¼yÃ¼m|zor|sevmem).*?(?:teknoloji|bilgisayar)',
                    r'programlama.*?(?:sevmiyorum|istemiyorum|zor|sevmem)'
                ],
                'filter_keywords': ['teknoloji', 'bilgisayar', 'yazÄ±lÄ±m', 'programlama', 'kodlama', 'matematik', 'sayÄ±sal', 'karmaÅŸÄ±k', 'zor', 'anlaÅŸÄ±lmaz']
            },
            'saÄŸlÄ±k': {
                'patterns': [
                    r'saÄŸlÄ±k.*?(?:istemiyorum|sevmiyorum|sevmem)',
                    r'kan.*?(?:gÃ¶rmek.*?istemiyorum|korkuyorum|sevmem)',
                    r'hasta.*?(?:gÃ¶rmek.*?istemiyorum|ilgilenmiyorum|sevmem)',
                    r'ameliyat.*?(?:korkuyorum|istemiyorum|sevmem)'
                ],
                'filter_keywords': ['saÄŸlÄ±k', 'hasta', 'kan', 'tÄ±p', 'ameliyat', 'hastalÄ±k', 'Ã¶lÃ¼m', 'acÄ±', 'korkutucu']
            },
            'matematik': {
                'patterns': [
                    r'matematik.*?(?:sevmiyorum|kÃ¶tÃ¼yÃ¼m|zor|anlayamÄ±yorum|sevmem)',
                    r'sayÄ±sal.*?(?:kÃ¶tÃ¼yÃ¼m|zor|baÅŸarÄ±sÄ±zÄ±m|sevmem)',
                    r'hesap.*?(?:yapmak.*?zor|sevmiyorum|sevmem)'
                ],
                'filter_keywords': ['matematik', 'hesap', 'sayÄ±', 'formÃ¼l', 'problem', 'Ã§Ã¶zÃ¼m', 'karmaÅŸÄ±k', 'zor']
            },
            'sosyal': {
                'patterns': [
                    r'tarih.*?(?:sevmiyorum|sÄ±kÄ±cÄ±|ezberleme|sevmem)',
                    r'edebiyat.*?(?:sevmiyorum|sÄ±kÄ±cÄ±|sevmem)',
                    r'ezberleme.*?(?:sevmiyorum|zor|sevmem)',
                    r'sosyal.*?(?:sevmiyorum|istemiyorum|sevmem)'
                ],
                'filter_keywords': ['tarih', 'edebiyat', 'ezber', 'okuma', 'yazma', 'analiz', 'sÄ±kÄ±cÄ±', 'yorucu']
            },
            'spor': {
                'patterns': [
                    r'spor.*?(?:sevmiyorum|istemiyorum|yapmam|sevmem)',
                    r'fiziksel.*?aktivite.*?(?:sevmiyorum|istemiyorum|sevmem)',
                    r'egzersiz.*?(?:sevmiyorum|yapmam|sevmem)',
                    r'tembel.*?(?:im|sayÄ±lÄ±rÄ±m)'
                ],
                'filter_keywords': ['spor', 'fitness', 'egzersiz', 'tembel', 'pasif', 'hareketsiz']
            },
            'iÅŸletme': {
                'patterns': [
                    r'iÅŸletme.*?(?:sevmiyorum|istemiyorum|sÄ±kÄ±cÄ±|sevmem)',
                    r'pazarlama.*?(?:sevmiyorum|istemiyorum|sevmem)',
                    r'muhasebe.*?(?:sevmiyorum|zor|sevmem)'
                ],
                'filter_keywords': ['iÅŸletme', 'pazarlama', 'muhasebe', 'sÄ±kÄ±cÄ±', 'karmaÅŸÄ±k', 'zor', 'stresli']
            },
            'eÄŸitim': {
                'patterns': [
                    r'Ã¶ÄŸretmen.*?(?:olmak.*?istemiyorum|sevmiyorum|sevmem)',
                    r'eÄŸitim.*?(?:sevmiyorum|istemiyorum|sÄ±kÄ±cÄ±|sevmem)',
                    r'Ã§ocuk.*?(?:sevmiyorum|ilgilenmiyorum|sevmem)'
                ],
                'filter_keywords': ['Ã¶ÄŸretmen', 'eÄŸitim', 'Ã§ocuk', 'sÄ±kÄ±cÄ±', 'zor', 'yorucu', 'stresli', 'ezber']
            }
        }
        
        # Check which categories to filter out
        categories_to_filter = set()
        for category, config in negative_filters.items():
            for pattern in config['patterns']:
                if re.search(pattern, user_lower):
                    categories_to_filter.add(category)
                    logger.info(f"ğŸš« Detected negative interest: {category}")
                    break
        
        # Filter results
        for result in results:
            idx = result['index']
            dept_row = self.departments_df.iloc[idx]
            dept_text = (dept_row['bolum_adi'] + ' ' + dept_row['Aciklama']).lower()
            
            # Check if department should be filtered out
            should_filter = False
            for category in categories_to_filter:
                filter_keywords = negative_filters[category]['filter_keywords']
                matching_keywords = [kw for kw in filter_keywords if kw in dept_text]
                
                if matching_keywords:
                    should_filter = True
                    logger.info(f"âŒ Filtered out: {dept_row['bolum_adi']} (negative: {category}, keywords: {matching_keywords})")
                    break
            
            if not should_filter:
                filtered_results.append(result)
        
        logger.info(f"ğŸ” Filtered from {len(results)} to {len(filtered_results)} departments")
        return filtered_results    
    
    def filter_by_ranking(self, ranking: int, tolerance_percent: float = 0.20):
        """Filter departments by ranking range with percentage-based tolerance"""
        if ranking is None:
            return self.departments_df.index.tolist()
        
        # YÃ¼zde bazlÄ± tolerance hesaplama yapÄ±larak tutarlÄ± sonuÃ§ saÄŸlanÄ±r
        tolerance_value = int(ranking * tolerance_percent)
        
        min_rank = max(1, ranking - tolerance_value)
        max_rank = ranking + tolerance_value
            
        filtered_indices = self.departments_df[
            (self.departments_df['ranking_2025'] >= min_rank) & 
            (self.departments_df['ranking_2025'] <= max_rank)
        ].index.tolist()
        
        logger.info(f"Ranking: {ranking}, Tolerance: %{tolerance_percent*100} = Â±{tolerance_value}")
        logger.info(f"Range: {min_rank} - {max_rank}")
        logger.info(f"Filtered to {len(filtered_indices)} departments")
        return filtered_indices
    
    def compute_semantic_similarity(self, interests: str, candidate_indices: list):
        """Compute semantic similarity between interests and candidate departments"""
        if not interests.strip():
            return []
            
        # Encode user interests
        interest_embedding = self.model.encode([interests])
        
        # Get embeddings for candidate departments
        candidate_embeddings = self.department_embeddings[candidate_indices]
        
        # Compute cosine similarity
        similarities = cosine_similarity(interest_embedding, candidate_embeddings)[0]
        
        # Create results with indices and scores
        results = []
        for i, idx in enumerate(candidate_indices):
            results.append({
                'index': idx,
                'similarity_score': similarities[i]
            })
            
        return results
    
    def boost_keyword_matches(self, interests: str, results: list):
        """Boost scores for exact keyword matches with expanded keywords"""
        interests_lower = interests.lower()
        
        # GeniÅŸletilmiÅŸ keyword mappings
        expanded_mappings = {
                'teknoloji': ['bilgisayar', 'yazÄ±lÄ±m', 'programlama', 'web', 'oyun', 'dijital', 'sistem', 'kodlama', 'geliÅŸtirme', 'uygulama', 'veri', 'yapay zeka', 'robotik', 'siber gÃ¼venlik'],
        'saÄŸlÄ±k': ['saÄŸlÄ±k', 'tÄ±p', 'hemÅŸire', 'hasta', 'tedavi', 'anestezi', 'veteriner', 'diÅŸ', 'fizyoterapi', 'tÄ±bbi', 'biyoloji', 'eczacÄ±lÄ±k', 'tÄ±bbi gÃ¶rÃ¼ntÃ¼leme'],
        'sanat': ['sanat', 'tasarÄ±m', 'grafik', 'mÃ¼zik', 'sinema', 'fotoÄŸraf', 'gÃ¶rsel', 'yaratÄ±cÄ±', 'illÃ¼strasyon', 'heykel', 'resim', 'seramik', 'tasarÄ±m', 'moda'],
        'spor': ['spor', 'antrenÃ¶r', 'fitness', 'egzersiz', 'rekreasyon', 'beden', 'atletik', 'hareket', 'yÃ¼zme', 'basketbol', 'futbol', 'tenis', 'spor yÃ¶netimi'],
        'iÅŸletme': ['iÅŸletme', 'pazarlama', 'muhasebe', 'ticaret', 'yÃ¶netim', 'ekonomi', 'finans', 'satÄ±ÅŸ', 'finansal', 'strateji', 'iÅŸ geliÅŸtirme', 'giriÅŸimcilik', 'ticaret', 'lojistik', 'insan kaynaklarÄ±'],
        'gastronomi': ['gastronomi', 'mutfak sanatlarÄ±', 'yemek', 'aÅŸÃ§Ä±lÄ±k', 'pasta', 'ÅŸef', 'fÄ±rÄ±ncÄ±lÄ±k', 'gÄ±da', 'restoran'],
        'eÄŸitim': ['Ã¶ÄŸretmen', 'eÄŸitim', 'Ã¶ÄŸretim', 'ders', 'okul', 'Ã§ocuk', 'akademik', 'Ã¶ÄŸrenci', 'pedagoji', 'psikoloji', 'rehberlik']
        }
        
        
        all_keywords = set()
        for word in interests_lower.split(','):
            word = word.strip()
            for category, expanded_keywords in expanded_mappings.items():
                if word in expanded_keywords[:3]:
                    all_keywords.update(expanded_keywords)
                    break
        
        # Keyword boost hesapla
        for result in results:
            idx = result['index']
            dept_row = self.departments_df.iloc[idx]
            dept_text = (dept_row['bolum_adi'] + ' ' + dept_row['Aciklama']).lower()
            
            keyword_boost = 0
            for keyword in all_keywords:
                if keyword in dept_text:
                    keyword_boost += 0.1
                    
            result['similarity_score'] += keyword_boost
            result['keyword_boost'] = keyword_boost
            
        return results
    
    def recommend(self, user_input: str, top_k: int = 10, tolerance_percent: float = 0.20):
        """Main recommendation function with percentage-based ranking tolerance"""
        logger.info(f"Processing recommendation for: {user_input}")
        def safe_taban_puan(value):
            if pd.isna(value):
                return None
            try:
                str_val = str(value)
                if ',' in str_val:
                    return None  # VirgÃ¼llÃ¼ deÄŸerleri skip et
                return float(str_val.replace('.', ''))
            except:
                return None

        # Parse input
        interests, ranking = self.extract_interests_and_ranking(user_input)
        logger.info(f"Extracted interests: '{interests}', ranking: {ranking}")
        
        # Filter by ranking with percentage tolerance
        candidate_indices = self.filter_by_ranking(ranking, tolerance_percent)
        
        if not candidate_indices:
            return []
            
        # Compute semantic similarity
        results = self.compute_semantic_similarity(interests, candidate_indices)
        
        # Boost keyword matches
        results = self.boost_keyword_matches(interests, results)

        # Filter negative interests
        results = self.filter_negative_interests(results, user_input)
        
        # Sort by score
        results.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        # Apply diversification
        results = self.diversify_by_department_type(results, top_k)
        
        # Prepare final recommendations
        recommendations = []
        for result in results:
            idx = result['index']
            dept_row = self.departments_df.iloc[idx]
            
            # Similarity_Prompt.py'de recommend fonksiyonunda

            recommendation = {
                'bolum_adi': dept_row['bolum_adi'],
                'universite': dept_row['Universite'],
                'sehir': dept_row['Sehir'],
                'ranking_2025': int(dept_row['ranking_2025']),  # Mevcut - deÄŸiÅŸtirme
                'taban_puan': None,  
                'similarity_score': round(result['similarity_score'], 4),
                'keyword_boost': round(result.get('keyword_boost', 0), 4),
                'description_preview': dept_row['Aciklama'][:150] + '...'
            }
            recommendations.append(recommendation)
            
        logger.info(f"Generated {len(recommendations)} recommendations")
        return recommendations
        
    def explain_recommendation(self, recommendation: dict):
        """Explain why this recommendation was made"""
        explanation = f"""
        BÃ¶lÃ¼m: {recommendation['bolum_adi']}
        Ãœniversite: {recommendation['universite']}
        SÄ±ralama: {recommendation['ranking_2025']}
        
        Benzerlik Skoru: {recommendation['similarity_score']:.4f}
        Anahtar Kelime Bonusu: {recommendation['keyword_boost']:.4f}
        
        AÃ§Ä±klama: {recommendation['description_preview']}
        """
        return explanation.strip()
    
    def diversify_by_department_type(self, results, top_k=6):
        """FarklÄ± bÃ¶lÃ¼m tÃ¼rlerinden seÃ§"""
        diverse_results = []
        used_dept_names = {}  # Dict ile count tutalÄ±m
        
        for result in results:
            idx = result['index']
            dept_name = self.departments_df.iloc[idx]['bolum_adi']
            
            # BÃ¶lÃ¼m adÄ±nÄ±n temel kÄ±smÄ±nÄ± al
            dept_base = dept_name.split('(')[0].split('-')[0].strip().upper()
            
            # Bu bÃ¶lÃ¼m tÃ¼rÃ¼nden kaÃ§ tane aldÄ±k kontrol et
            current_count = used_dept_names.get(dept_base, 0)
            
            # AynÄ± bÃ¶lÃ¼m tÃ¼rÃ¼nden maksimum 2 tane al
            if current_count < 2:
                diverse_results.append(result)
                used_dept_names[dept_base] = current_count + 1
                
                logger.info(f"Added: {dept_base} (Count: {current_count + 1})")
            else:
                logger.info(f"Skipped: {dept_base} (Already have {current_count})")
                
            if len(diverse_results) >= top_k:
                break
        
        return diverse_results
def main():
    """Test the recommendation engine"""
    
    # Initialize engine
    dataset_path = "./Backend/Data/2yillik_Bolumler_aciklamali_yeni.csv"
    engine = HybridRecommendationEngine(dataset_path)
    
    # Test cases
    test_cases = [
        "saÄŸlÄ±k alanÄ±nda Ã§alÄ±ÅŸmak istiyorum ama sayÄ±sal iyi deÄŸilim iÃ§inde teknolojik bir ÅŸey olmasÄ±n sÄ±ralamam 500.000"
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"TEST {i}: {test_case}")
        print('='*80)
        
        recommendations = engine.recommend(test_case, top_k=6)
        
        for j, rec in enumerate(recommendations, 1):
            print(f"{j}. {rec['bolum_adi']} - {rec['universite']}")
            print(f"   SÄ±ralama: {rec['ranking_2025']}")
            print(f"   Benzerlik: {rec['similarity_score']:.4f}")

if __name__ == "__main__":
    main()